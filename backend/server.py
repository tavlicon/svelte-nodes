"""
Generative Design Studio Backend Server
Provides REST API for:
- Stable Diffusion 1.5 img2img generation
- TripoSR single-image to 3D mesh generation
"""

import asyncio
import base64
import io
import json
import os
import time
import uuid
import logging
import ssl
import certifi
from pathlib import Path
from typing import Optional

import torch
import numpy as np
from PIL import Image
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse
from pydantic import BaseModel
from sse_starlette.sse import EventSourceResponse

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Fix SSL certificate issues for any network calls
os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()
os.environ['SSL_CERT_FILE'] = certifi.where()
os.environ['CURL_CA_BUNDLE'] = certifi.where()

# Diffusers imports
from diffusers import (
    StableDiffusionImg2ImgPipeline,
    DDIMScheduler,
    EulerDiscreteScheduler,
    EulerAncestralDiscreteScheduler,
    DPMSolverMultistepScheduler,
    LCMScheduler,
    UniPCMultistepScheduler,
    AutoencoderKL,
)
from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput
from transformers import CLIPTextModel, CLIPTokenizer

app = FastAPI(title="Generative Design Studio Backend", version="1.0.0")

# Debug instrumentation configuration (do not remove until post-fix verification)
DEBUG_LOG_PATH = Path("/Users/olicon/github/generative-design-studio-2/.cursor/debug.log")

# CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global pipeline storage
pipeline: Optional[StableDiffusionImg2ImgPipeline] = None
current_device = "cpu"
model_loaded = False

# TripoSR model storage
triposr_model = None
triposr_loaded = False

# Progress tracking
progress_updates: dict[str, list] = {}


class Img2ImgRequest(BaseModel):
    """Request model for img2img generation"""
    positive_prompt: str
    negative_prompt: str = ""
    seed: int = 42
    steps: int = 20
    cfg: float = 7.5
    sampler_name: str = "euler"
    scheduler: str = "normal"
    denoise: float = 0.75
    width: int = 512
    height: int = 512


class ModelInfo(BaseModel):
    """Model information response"""
    loaded: bool
    model_name: str = ""
    device: str = "cpu"


# Scheduler mapping
SCHEDULERS = {
    "lcm": LCMScheduler,
    "euler": EulerDiscreteScheduler,
    "euler_a": EulerAncestralDiscreteScheduler,
    "dpmpp_2m": DPMSolverMultistepScheduler,
    "dpmpp_2m_sde": DPMSolverMultistepScheduler,
    "dpmpp_sde": DPMSolverMultistepScheduler,
    "ddim": DDIMScheduler,
    "uni_pc": UniPCMultistepScheduler,
}


def get_scheduler(sampler_name: str, scheduler_type: str, config):
    """Get the appropriate scheduler based on sampler name and type"""
    scheduler_class = SCHEDULERS.get(sampler_name, EulerDiscreteScheduler)
    
    # Create scheduler with appropriate config
    scheduler_config = dict(config)
    
    # Handle karras sigmas for applicable schedulers
    if scheduler_type == "karras" and sampler_name in ["euler", "euler_a", "dpmpp_2m", "dpmpp_sde"]:
        scheduler_config["use_karras_sigmas"] = True
    
    # Handle SDE variants
    if "sde" in sampler_name:
        scheduler_config["algorithm_type"] = "sde-dpmsolver++"
    
    return scheduler_class.from_config(scheduler_config)


def get_device_and_dtype():
    """Determine the best device and dtype for inference"""
    if torch.cuda.is_available():
        return "cuda", torch.float16
    elif torch.backends.mps.is_available():
        # MPS works best with float32 to avoid NaN issues in VAE
        return "mps", torch.float32
    else:
        return "cpu", torch.float32


def load_model_local(model_path: str):
    """Load from local diffusers directory - FULLY OFFLINE, no network"""
    global pipeline, current_device, model_loaded
    
    logger.info(f"Loading model from local directory: {model_path}")
    
    current_device, torch_dtype = get_device_and_dtype()
    logger.info(f"Using device: {current_device}, dtype: {torch_dtype}")
    
    try:
        # Load from local diffusers format - no network needed
        pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
            model_path,
            torch_dtype=torch_dtype,
            local_files_only=True,  # Strictly offline
            safety_checker=None,
            requires_safety_checker=False,
        )
        
        logger.info(f"Moving pipeline to {current_device}...")
        pipeline = pipeline.to(current_device)
        
        # Enable memory optimizations
        if current_device == "cuda":
            pipeline.enable_attention_slicing()
            try:
                pipeline.enable_xformers_memory_efficient_attention()
                logger.info("Enabled xformers memory efficient attention")
            except Exception:
                pass
        elif current_device == "mps":
            pipeline.enable_attention_slicing()
            logger.info("Enabled attention slicing for MPS")
            logger.info("Pipeline running in float32 for MPS stability")
        
        model_loaded = True
        logger.info("‚úÖ Model loaded successfully (offline mode)!")
        
    except Exception as e:
        logger.error(f"‚ùå Error loading model: {e}")
        import traceback
        traceback.print_exc()
        raise


def load_model_from_safetensors(model_path: str):
    """Load from safetensors file - requires network for config on first run"""
    global pipeline, current_device, model_loaded
    
    logger.info(f"Loading model from safetensors: {model_path}")
    
    current_device, torch_dtype = get_device_and_dtype()
    logger.info(f"Using device: {current_device}, dtype: {torch_dtype}")
    
    try:
        # This will download config from HuggingFace on first run
        pipeline = StableDiffusionImg2ImgPipeline.from_single_file(
            model_path,
            torch_dtype=torch_dtype,
            use_safetensors=True,
            load_safety_checker=False,
        )
        
        logger.info(f"Moving pipeline to {current_device}...")
        pipeline = pipeline.to(current_device)
        
        # Disable safety checker
        pipeline.safety_checker = None
        pipeline.requires_safety_checker = False
        
        # Enable memory optimizations
        if current_device == "cuda":
            pipeline.enable_attention_slicing()
            try:
                pipeline.enable_xformers_memory_efficient_attention()
                logger.info("Enabled xformers memory efficient attention")
            except Exception:
                pass
        elif current_device == "mps":
            pipeline.enable_attention_slicing()
            logger.info("Enabled attention slicing for MPS")
        
        model_loaded = True
        logger.info("‚úÖ Model loaded successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå Error loading model: {e}")
        import traceback
        traceback.print_exc()
        raise


def image_to_base64(image: Image.Image, format: str = "PNG") -> str:
    """Convert PIL Image to base64 string"""
    buffer = io.BytesIO()
    image.save(buffer, format=format)
    return base64.b64encode(buffer.getvalue()).decode()


def base64_to_image(base64_str: str) -> Image.Image:
    """Convert base64 string to PIL Image"""
    # Handle data URL format
    if base64_str.startswith("data:"):
        base64_str = base64_str.split(",")[1]
    
    image_data = base64.b64decode(base64_str)
    return Image.open(io.BytesIO(image_data))


@app.on_event("startup")
async def startup_event():
    """Load model on startup - LOCAL ONLY MODE"""
    global pipeline, current_device, model_loaded
    
    # Look for model in data/models directory
    model_dir = Path(__file__).parent.parent / "data" / "models"
    local_model_path = model_dir / "sd-v1-5-local"  # Diffusers format (preferred)
    safetensors_path = model_dir / "v1-5-pruned-emaonly-fp16.safetensors"
    
    logger.info("=" * 60)
    logger.info("üöÄ Starting Stable Diffusion Backend Server")
    logger.info("üìç Mode: LOCAL ONLY (no network calls)")
    logger.info("=" * 60)

    # region agent log
    try:
        DEBUG_LOG_PATH.parent.mkdir(parents=True, exist_ok=True)
        DEBUG_LOG_PATH.open("a").write(json.dumps({
            "sessionId": "debug-session",
            "runId": "pre-fix",
            "hypothesisId": "H1",
            "location": "backend/server.py:startup_event",
            "message": "Startup model presence check",
            "data": {
                "localModelExists": local_model_path.exists(),
                "safetensorsExists": safetensors_path.exists(),
                "cwd": os.getcwd()
            },
            "timestamp": int(time.time() * 1000)
        }) + "\n")
    except Exception:
        pass
    # endregion
    
    # Prefer local diffusers format (fully offline)
    if local_model_path.exists():
        logger.info(f"Found local model directory: {local_model_path}")
        try:
            load_model_local(str(local_model_path))
        except Exception as e:
            logger.error(f"Failed to load local model: {e}")
            logger.warning("‚ö†Ô∏è Server starting without model")
    
    # Fallback to safetensors (requires network for config on first run)
    elif safetensors_path.exists():
        logger.warning(f"Found safetensors file, but no local model directory.")
        logger.warning(f"Please run: python setup_model.py")
        logger.warning(f"This downloads model config once, then works offline.")
        logger.info("")
        logger.info("Attempting to load from safetensors (may require network)...")
        
        # Temporarily disable offline mode for first-time setup
        os.environ.pop('HF_HUB_OFFLINE', None)
        os.environ.pop('TRANSFORMERS_OFFLINE', None)
        
        try:
            load_model_from_safetensors(str(safetensors_path))
            # Save for next time
            if model_loaded and pipeline is not None:
                logger.info(f"Saving model locally for offline use...")
                pipeline.save_pretrained(str(local_model_path))
                logger.info(f"‚úÖ Model saved to {local_model_path}")
        except Exception as e:
            logger.error(f"Failed to load from safetensors: {e}")
            logger.warning("‚ö†Ô∏è Server starting without model")
            logger.warning("Run 'python setup_model.py' to set up the model.")
    else:
        logger.error(f"‚ùå No model found!")
        logger.error(f"  - Local model: {local_model_path}")
        logger.error(f"  - Safetensors: {safetensors_path}")
        logger.warning("‚ö†Ô∏è Server starting without model")
    
    logger.info("=" * 60)
    if model_loaded:
        logger.info(f"‚úÖ Ready for inference on {current_device.upper()}")
    else:
        logger.warning("‚ö†Ô∏è Model NOT loaded - inference will fail")
    logger.info("=" * 60)

    # region agent log
    try:
        DEBUG_LOG_PATH.open("a").write(json.dumps({
            "sessionId": "debug-session",
            "runId": "pre-fix",
            "hypothesisId": "H1",
            "location": "backend/server.py:startup_event",
            "message": "Startup model load result",
            "data": {
                "modelLoaded": model_loaded,
                "device": current_device,
                "localModelPath": str(local_model_path),
                "safetensorsPath": str(safetensors_path)
            },
            "timestamp": int(time.time() * 1000)
        }) + "\n")
    except Exception:
        pass
    # endregion


@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "ok", "model_loaded": model_loaded}


@app.get("/api/model/info", response_model=ModelInfo)
async def get_model_info():
    """Get current model information"""
    return ModelInfo(
        loaded=model_loaded,
        model_name="v1-5-pruned-emaonly-fp16" if model_loaded else "",
        device=current_device,
    )


@app.post("/api/model/load")
async def load_model_endpoint(model_path: str = Form(...)):
    """Load a model from the specified path"""
    global model_loaded
    
    if not os.path.exists(model_path):
        raise HTTPException(status_code=404, detail=f"Model not found: {model_path}")
    
    try:
        load_model(model_path)
        return {"status": "success", "device": current_device}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/img2img")
async def img2img(
    image: UploadFile = File(...),
    positive_prompt: str = Form(...),
    negative_prompt: str = Form(""),
    seed: int = Form(42),
    steps: int = Form(20),
    cfg: float = Form(7.5),
    sampler_name: str = Form("euler"),
    scheduler: str = Form("normal"),
    denoise: float = Form(0.75),
):
    """
    Perform img2img generation
    Returns the generated image as base64
    """
    global pipeline
    
    logger.info("=" * 50)
    logger.info("üì• Received img2img request")
    logger.info(f"  Prompt: {positive_prompt[:50]}...")
    logger.info(f"  Negative: {negative_prompt[:30]}..." if negative_prompt else "  Negative: (none)")
    logger.info(f"  Steps: {steps}, CFG: {cfg}, Denoise: {denoise}")
    logger.info(f"  Sampler: {sampler_name}, Scheduler: {scheduler}")
    logger.info(f"  Seed: {seed}")
    
    if not model_loaded or pipeline is None:
        logger.error("‚ùå Model not loaded!")
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # Validate parameter combination - ensure at least 1 effective inference step
    effective_steps = int(steps * denoise)
    if effective_steps < 1:
        error_msg = f"Invalid parameter combination: steps={steps} √ó denoise={denoise} = {effective_steps} effective steps. Need at least 1. Try increasing steps (‚â•10) or denoise (‚â•0.1)."
        logger.error(f"‚ùå {error_msg}")
        raise HTTPException(status_code=400, detail=error_msg)
    
    try:
        # Read and process input image
        logger.info("üì∑ Processing input image...")
        image_data = await image.read()
        input_image = Image.open(io.BytesIO(image_data)).convert("RGB")
        logger.info(f"  Input size: {input_image.width}x{input_image.height}")
        
        # Resize to target dimensions (must be divisible by 8)
        # Cap maximum dimension to prevent memory issues (SD 1.5 works best at 512-768)
        MAX_DIMENSION = 768
        
        width = input_image.width
        height = input_image.height
        
        # Scale down if too large while maintaining aspect ratio
        if width > MAX_DIMENSION or height > MAX_DIMENSION:
            scale = MAX_DIMENSION / max(width, height)
            width = int(width * scale)
            height = int(height * scale)
            logger.info(f"  Scaling down to fit {MAX_DIMENSION}px max dimension")
        
        # Make divisible by 8
        width = (width // 8) * 8
        height = (height // 8) * 8
        
        if width != input_image.width or height != input_image.height:
            input_image = input_image.resize((width, height), Image.Resampling.LANCZOS)
            logger.info(f"  Resized to: {width}x{height}")
        
        # Set up scheduler
        logger.info(f"‚öôÔ∏è Setting up scheduler: {sampler_name} ({scheduler})")
        pipeline.scheduler = get_scheduler(
            sampler_name, 
            scheduler, 
            pipeline.scheduler.config
        )
        
        # Set seed for reproducibility
        generator = torch.Generator(device=current_device)
        if seed >= 0:
            generator = generator.manual_seed(seed)
            logger.info(f"  Using seed: {seed}")
        else:
            random_seed = torch.randint(0, 2**32 - 1, (1,)).item()
            generator = generator.manual_seed(random_seed)
            logger.info(f"  Using random seed: {random_seed}")
        
        # Run inference
        logger.info("üöÄ Starting inference...")
        start_time = time.time()
        
        result = pipeline(
            prompt=positive_prompt,
            negative_prompt=negative_prompt,
            image=input_image,
            strength=denoise,
            num_inference_steps=steps,
            guidance_scale=cfg,
            generator=generator,
        )
        
        elapsed_time = time.time() - start_time
        logger.info(f"‚úÖ Inference complete in {elapsed_time:.2f}s")
        
        # Get output image
        output_image = result.images[0]
        logger.info(f"  Output size: {output_image.width}x{output_image.height}")
        
        # Debug: Check if output is black
        import numpy as np
        output_array = np.array(output_image)
        logger.info(f"  Output stats: min={output_array.min()}, max={output_array.max()}, mean={output_array.mean():.2f}")
        if output_array.max() == 0:
            logger.warning("‚ö†Ô∏è Output image is all black! This indicates a VAE decoding issue.")
        
        # Save to output directory
        output_dir = Path(__file__).parent.parent / "data" / "output"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_filename = f"img2img_{int(time.time())}_{seed}.png"
        output_path = output_dir / output_filename
        output_image.save(output_path)
        logger.info(f"üíæ Saved to: {output_path}")
        
        # Convert to base64
        output_base64 = image_to_base64(output_image)
        
        logger.info("=" * 50)
        
        return {
            "status": "success",
            "image": f"data:image/png;base64,{output_base64}",
            "time_taken": elapsed_time,
            "width": output_image.width,
            "height": output_image.height,
            "output_path": str(output_path),
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error during inference: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/img2img/stream")
async def img2img_stream(
    image: UploadFile = File(...),
    positive_prompt: str = Form(...),
    negative_prompt: str = Form(""),
    seed: int = Form(42),
    steps: int = Form(20),
    cfg: float = Form(7.5),
    sampler_name: str = Form("euler"),
    scheduler: str = Form("normal"),
    denoise: float = Form(0.75),
):
    """
    Perform img2img generation with SSE progress streaming
    """
    global pipeline
    
    if not model_loaded or pipeline is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # Validate parameter combination - ensure at least 1 effective inference step
    effective_steps = int(steps * denoise)
    if effective_steps < 1:
        error_msg = f"Invalid parameter combination: steps={steps} √ó denoise={denoise} = {effective_steps} effective steps. Need at least 1. Try increasing steps (‚â•10) or denoise (‚â•0.1)."
        raise HTTPException(status_code=400, detail=error_msg)
    
    request_id = str(uuid.uuid4())
    progress_updates[request_id] = []
    
    async def generate():
        try:
            # Read and process input image
            image_data = await image.read()
            input_image = Image.open(io.BytesIO(image_data)).convert("RGB")
            
            # Resize to target dimensions
            # Cap maximum dimension to prevent memory issues
            MAX_DIMENSION = 768
            
            width = input_image.width
            height = input_image.height
            
            # Scale down if too large while maintaining aspect ratio
            if width > MAX_DIMENSION or height > MAX_DIMENSION:
                scale = MAX_DIMENSION / max(width, height)
                width = int(width * scale)
                height = int(height * scale)
            
            # Make divisible by 8
            width = (width // 8) * 8
            height = (height // 8) * 8
            
            if width != input_image.width or height != input_image.height:
                input_image = input_image.resize((width, height), Image.Resampling.LANCZOS)
            
            # Set up scheduler
            pipeline.scheduler = get_scheduler(
                sampler_name,
                scheduler,
                pipeline.scheduler.config
            )
            
            # Set seed
            generator = torch.Generator(device=current_device)
            if seed >= 0:
                generator = generator.manual_seed(seed)
            else:
                generator = generator.manual_seed(torch.randint(0, 2**32 - 1, (1,)).item())
            
            # Progress callback
            def progress_callback(step: int, timestep: int, latents: torch.Tensor):
                progress_data = {
                    "step": step + 1,
                    "total_steps": steps,
                    "progress": (step + 1) / steps * 100,
                }
                progress_updates[request_id].append(progress_data)
            
            # Send initial progress
            yield {
                "event": "progress",
                "data": json.dumps({"step": 0, "total_steps": steps, "progress": 0, "status": "starting"})
            }
            
            start_time = time.time()
            
            # Run inference with callback
            result = pipeline(
                prompt=positive_prompt,
                negative_prompt=negative_prompt,
                image=input_image,
                strength=denoise,
                num_inference_steps=steps,
                guidance_scale=cfg,
                generator=generator,
                callback=progress_callback,
                callback_steps=1,
            )
            
            elapsed_time = time.time() - start_time
            
            # Get output image
            output_image = result.images[0]
            output_base64 = image_to_base64(output_image)
            
            # Send completion
            yield {
                "event": "complete",
                "data": json.dumps({
                    "status": "success",
                    "image": f"data:image/png;base64,{output_base64}",
                    "time_taken": elapsed_time,
                    "width": output_image.width,
                    "height": output_image.height,
                })
            }
            
        except Exception as e:
            yield {
                "event": "error",
                "data": json.dumps({"status": "error", "message": str(e)})
            }
        finally:
            # Cleanup
            if request_id in progress_updates:
                del progress_updates[request_id]
    
    return EventSourceResponse(generate())


@app.post("/api/img2img/base64")
async def img2img_base64(
    image_base64: str = Form(...),
    positive_prompt: str = Form(...),
    negative_prompt: str = Form(""),
    seed: int = Form(42),
    steps: int = Form(20),
    cfg: float = Form(7.5),
    sampler_name: str = Form("euler"),
    scheduler: str = Form("normal"),
    denoise: float = Form(0.75),
):
    """
    Perform img2img generation with base64 input image
    """
    global pipeline
    
    if not model_loaded or pipeline is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # Validate parameter combination - ensure at least 1 effective inference step
    effective_steps = int(steps * denoise)
    if effective_steps < 1:
        error_msg = f"Invalid parameter combination: steps={steps} √ó denoise={denoise} = {effective_steps} effective steps. Need at least 1. Try increasing steps (‚â•10) or denoise (‚â•0.1)."
        raise HTTPException(status_code=400, detail=error_msg)
    
    try:
        # Decode input image from base64
        input_image = base64_to_image(image_base64).convert("RGB")
        
        # Resize to target dimensions (must be divisible by 8)
        # Cap maximum dimension to prevent memory issues
        MAX_DIMENSION = 768
        
        width = input_image.width
        height = input_image.height
        
        # Scale down if too large while maintaining aspect ratio
        if width > MAX_DIMENSION or height > MAX_DIMENSION:
            scale = MAX_DIMENSION / max(width, height)
            width = int(width * scale)
            height = int(height * scale)
        
        # Make divisible by 8
        width = (width // 8) * 8
        height = (height // 8) * 8
        
        if width != input_image.width or height != input_image.height:
            input_image = input_image.resize((width, height), Image.Resampling.LANCZOS)
        
        # Set up scheduler
        pipeline.scheduler = get_scheduler(
            sampler_name,
            scheduler,
            pipeline.scheduler.config
        )
        
        # Set seed for reproducibility
        generator = torch.Generator(device=current_device)
        if seed >= 0:
            generator = generator.manual_seed(seed)
        else:
            generator = generator.manual_seed(torch.randint(0, 2**32 - 1, (1,)).item())
        
        # Run inference
        start_time = time.time()
        
        result = pipeline(
            prompt=positive_prompt,
            negative_prompt=negative_prompt,
            image=input_image,
            strength=denoise,
            num_inference_steps=steps,
            guidance_scale=cfg,
            generator=generator,
        )
        
        elapsed_time = time.time() - start_time
        
        # Get output image
        output_image = result.images[0]
        
        # Convert to base64
        output_base64 = image_to_base64(output_image)
        
        return {
            "status": "success",
            "image": f"data:image/png;base64,{output_base64}",
            "time_taken": elapsed_time,
            "width": output_image.width,
            "height": output_image.height,
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# =============================================================================
# TripoSR 3D Mesh Generation
# =============================================================================

class TripoSRRequest(BaseModel):
    """Request model for TripoSR 3D generation"""
    foreground_ratio: float = 0.85
    mc_resolution: int = 256
    chunk_size: int = 8192


def load_triposr_model():
    """Load TripoSR model from local directory"""
    global triposr_model, triposr_loaded, current_device
    
    if triposr_loaded and triposr_model is not None:
        return True
    
    model_dir = Path(__file__).parent.parent / "data" / "models"
    triposr_path = model_dir / "triposr-base"
    dino_path = model_dir / "dino-vitb16"
    
    if not triposr_path.exists():
        logger.error(f"‚ùå TripoSR model not found at {triposr_path}")
        return False
    
    if not dino_path.exists():
        logger.error(f"‚ùå DINO encoder not found at {dino_path}")
        return False
    
    logger.info("=" * 60)
    logger.info("üî∫ Loading TripoSR model...")
    
    try:
        # Import TripoSR components
        from omegaconf import OmegaConf
        from transformers import ViTImageProcessor, ViTModel
        
        # Determine device
        device, torch_dtype = get_device_and_dtype()
        # TripoSR works better with float32 on MPS
        if device == "mps":
            torch_dtype = torch.float32
        
        logger.info(f"  Device: {device}, dtype: {torch_dtype}")
        
        # Load config
        config_path = triposr_path / "config.yaml"
        config = OmegaConf.load(config_path)
        logger.info(f"  Loaded config from {config_path}")
        
        # Load DINO image processor and model
        logger.info(f"  Loading DINO encoder from {dino_path}...")
        dino_processor = ViTImageProcessor.from_pretrained(str(dino_path), local_files_only=True)
        dino_model = ViTModel.from_pretrained(str(dino_path), local_files_only=True)
        dino_model = dino_model.to(device)
        dino_model.eval()
        
        # Load TripoSR checkpoint
        logger.info(f"  Loading TripoSR checkpoint...")
        ckpt_path = triposr_path / "model.ckpt"
        checkpoint = torch.load(ckpt_path, map_location=device, weights_only=False)
        
        # Store model components
        triposr_model = {
            "config": config,
            "dino_processor": dino_processor,
            "dino_model": dino_model,
            "checkpoint": checkpoint,
            "device": device,
            "dtype": torch_dtype,
        }
        
        triposr_loaded = True
        logger.info("‚úÖ TripoSR model loaded successfully!")
        logger.info("=" * 60)
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load TripoSR model: {e}")
        import traceback
        traceback.print_exc()
        return False


def remove_background(image: Image.Image, foreground_ratio: float = 0.85) -> Image.Image:
    """Remove background from image using rembg"""
    try:
        from rembg import remove
        
        # Remove background
        image_rgba = remove(image)
        
        # Create white background
        background = Image.new("RGBA", image_rgba.size, (255, 255, 255, 255))
        
        # Composite
        result = Image.alpha_composite(background, image_rgba)
        
        # Crop to foreground
        bbox = image_rgba.getbbox()
        if bbox:
            # Add padding
            w, h = image_rgba.size
            cx, cy = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2
            max_dim = max(bbox[2] - bbox[0], bbox[3] - bbox[1])
            
            # Calculate crop with foreground ratio
            crop_size = max_dim / foreground_ratio
            half_size = crop_size / 2
            
            left = max(0, int(cx - half_size))
            top = max(0, int(cy - half_size))
            right = min(w, int(cx + half_size))
            bottom = min(h, int(cy + half_size))
            
            result = result.crop((left, top, right, bottom))
        
        return result.convert("RGB")
        
    except ImportError:
        logger.warning("rembg not installed, skipping background removal")
        return image


def process_triposr_image(image: Image.Image, target_size: int = 512) -> Image.Image:
    """Process image for TripoSR: resize to square"""
    # Resize to target size maintaining aspect ratio, then center crop
    w, h = image.size
    scale = target_size / min(w, h)
    new_w, new_h = int(w * scale), int(h * scale)
    image = image.resize((new_w, new_h), Image.Resampling.LANCZOS)
    
    # Center crop to square
    left = (new_w - target_size) // 2
    top = (new_h - target_size) // 2
    image = image.crop((left, top, left + target_size, top + target_size))
    
    return image


def extract_mesh_marching_cubes(
    scene_features: torch.Tensor,
    checkpoint: dict,
    device: str,
    resolution: int = 256,
    chunk_size: int = 8192
) -> "trimesh.Trimesh":
    """
    Extract 3D mesh using marching cubes algorithm.
    This is a simplified implementation - full TripoSR uses more sophisticated extraction.
    """
    import trimesh
    from skimage import measure
    
    logger.info(f"  Extracting mesh at resolution {resolution}...")
    
    # Create a simple unit sphere mesh as placeholder for actual TripoSR mesh extraction
    # In production, this would use the NeRF decoder and marching cubes
    
    # For now, create a simple mesh from the features
    # This placeholder will be replaced with actual TripoSR mesh extraction
    
    # Generate a basic mesh (sphere for testing)
    mesh = trimesh.creation.icosphere(subdivisions=3, radius=0.5)
    
    return mesh


@app.get("/api/triposr/info")
async def get_triposr_info():
    """Get TripoSR model status"""
    return {
        "loaded": triposr_loaded,
        "device": triposr_model["device"] if triposr_model else "none",
        "model_name": "TripoSR Base",
    }


@app.post("/api/triposr/load")
async def load_triposr_endpoint():
    """Explicitly load TripoSR model"""
    success = load_triposr_model()
    if success:
        return {"status": "success", "device": triposr_model["device"]}
    else:
        raise HTTPException(status_code=500, detail="Failed to load TripoSR model")


@app.post("/api/triposr")
async def generate_3d_mesh(
    image: UploadFile = File(...),
    foreground_ratio: float = Form(0.85),
    mc_resolution: int = Form(256),
    remove_bg: bool = Form(True),
):
    """
    Generate 3D mesh from single image using TripoSR.
    Returns path to generated GLB file.
    """
    logger.info("=" * 50)
    logger.info("üì• Received TripoSR request")
    logger.info(f"  Foreground ratio: {foreground_ratio}")
    logger.info(f"  MC Resolution: {mc_resolution}")
    logger.info(f"  Remove background: {remove_bg}")
    
    # Load model if not loaded
    if not triposr_loaded:
        logger.info("  Loading TripoSR model (first request)...")
        success = load_triposr_model()
        if not success:
            raise HTTPException(status_code=503, detail="TripoSR model not available")
    
    try:
        import trimesh
        
        # Read and process input image
        logger.info("üì∑ Processing input image...")
        image_data = await image.read()
        input_image = Image.open(io.BytesIO(image_data)).convert("RGB")
        logger.info(f"  Input size: {input_image.width}x{input_image.height}")
        
        # Remove background if requested
        if remove_bg:
            logger.info("  Removing background...")
            input_image = remove_background(input_image, foreground_ratio)
            logger.info(f"  After background removal: {input_image.width}x{input_image.height}")
        
        # Process to 512x512
        input_image = process_triposr_image(input_image, 512)
        logger.info(f"  Processed size: {input_image.width}x{input_image.height}")
        
        start_time = time.time()
        
        # Get DINO features
        logger.info("üß† Extracting DINO features...")
        device = triposr_model["device"]
        dino_processor = triposr_model["dino_processor"]
        dino_model = triposr_model["dino_model"]
        
        # Process image through DINO
        inputs = dino_processor(images=input_image, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = dino_model(**inputs)
            image_features = outputs.last_hidden_state
        
        logger.info(f"  Features shape: {image_features.shape}")
        
        # Extract mesh using marching cubes
        logger.info("üî∫ Generating 3D mesh...")
        mesh = extract_mesh_marching_cubes(
            image_features,
            triposr_model["checkpoint"],
            device,
            resolution=mc_resolution,
        )
        
        elapsed_time = time.time() - start_time
        logger.info(f"‚úÖ Mesh generation complete in {elapsed_time:.2f}s")
        logger.info(f"  Vertices: {len(mesh.vertices)}, Faces: {len(mesh.faces)}")
        
        # Save mesh as GLB
        output_dir = Path(__file__).parent.parent / "data" / "output"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_filename = f"mesh_{int(time.time())}_{uuid.uuid4().hex[:8]}.glb"
        output_path = output_dir / output_filename
        
        # Export to GLB
        mesh.export(str(output_path), file_type="glb")
        logger.info(f"üíæ Saved to: {output_path}")
        
        # Also save a rendered preview image
        preview_filename = output_filename.replace(".glb", "_preview.png")
        preview_path = output_dir / preview_filename
        
        # Create a simple preview by rendering the mesh
        try:
            scene = mesh.scene()
            # Get PNG data from scene
            png_data = scene.save_image(resolution=(512, 512))
            with open(preview_path, 'wb') as f:
                f.write(png_data)
            logger.info(f"  Preview saved to: {preview_path}")
            preview_url = f"/data/output/{preview_filename}"
        except Exception as e:
            logger.warning(f"  Could not generate preview: {e}")
            preview_url = None
        
        logger.info("=" * 50)
        
        return {
            "status": "success",
            "mesh_path": f"/data/output/{output_filename}",
            "preview_url": preview_url,
            "output_path": str(output_path),
            "time_taken": elapsed_time,
            "vertices": len(mesh.vertices),
            "faces": len(mesh.faces),
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error during TripoSR inference: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
