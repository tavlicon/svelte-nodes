# Generative Design Studio

A node-based canvas editor for building AI image generation workflows. Built with Svelte 5, WebGPU, and a Python backend using Diffusers.

## Features

- **Node-Based Canvas**: GPU-accelerated infinite canvas with drag-and-drop node creation
- **WebGPU/Canvas2D Rendering**: Hardware-accelerated rendering with automatic fallback
- **Visual Connections**: Bezier curve connectors with snap-to-port and type validation
- **SD 1.5 img2img**: Local Stable Diffusion inference using Diffusers library
- **Output Node**: Auto-generated output node shows result with file path
- **Asset Management**: Drag and drop images and models from sidebar or desktop
- **Undo/Redo**: 5-level history with keyboard shortcuts (âŒ˜Z / â‡§âŒ˜Z)
- **Light/Dark Theme**: Toggle between themes with persistent preference
- **Local File Storage**: All assets stored locally in `data/` directory

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GENERATIVE DESIGN STUDIO                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   FRONTEND       â”‚  HTTP   â”‚   BACKEND (Python)           â”‚ â”‚
â”‚  â”‚   (Svelte 5)     â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚   FastAPI + Diffusers        â”‚ â”‚
â”‚  â”‚                  â”‚         â”‚                              â”‚ â”‚
â”‚  â”‚  â€¢ Node Editor   â”‚         â”‚  â€¢ Loads SD 1.5 model        â”‚ â”‚
â”‚  â”‚  â€¢ WebGPU Canvas â”‚         â”‚  â€¢ Runs on MPS/CUDA/CPU      â”‚ â”‚
â”‚  â”‚  â€¢ Parameters    â”‚         â”‚  â€¢ img2img inference         â”‚ â”‚
â”‚  â”‚  â€¢ Output Displayâ”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  â€¢ Returns generated image   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        localhost:5173              localhost:8000              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Note**: This is a standalone application. It does NOT use ComfyUI or any external inference service.

## Requirements

### Frontend
- Node.js 18+
- Modern browser with WebGPU support (recommended):
  - Chrome 113+, Edge 113+, Safari 18+
- Falls back to Canvas 2D for older browsers

### Backend
- Python 3.10+
- ~8GB RAM (for model loading)
- GPU recommended:
  - **Apple Silicon** (M1/M2/M3): Uses MPS acceleration
  - **NVIDIA GPU**: Uses CUDA acceleration
  - **CPU**: Works but slower (~30s/image)

## Quick Start

### 1. Clone and Install Frontend

```bash
git clone https://github.com/yourusername/generative-design-studio-2.git
cd generative-design-studio-2

# Install frontend dependencies
npm install
```

### 2. Set Up Python Backend

```bash
cd backend
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Download Model Files (Offline Setup)

For fully offline operation, download these files from HuggingFace and place them in `data/models/sd-v1-5-local/`:

```
data/models/sd-v1-5-local/
â”œâ”€â”€ model_index.json
â”œâ”€â”€ scheduler/
â”‚   â””â”€â”€ scheduler_config.json
â”œâ”€â”€ text_encoder/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ model.safetensors (~492 MB)
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ merges.txt
â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ vocab.json
â”œâ”€â”€ unet/
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ diffusion_pytorch_model.safetensors (~3.4 GB)
â””â”€â”€ vae/
    â”œâ”€â”€ config.json
    â””â”€â”€ diffusion_pytorch_model.safetensors (~335 MB)
```

Download links (from `runwayml/stable-diffusion-v1-5`):
- All config files: https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main
- Model weights: Download the `.safetensors` files for each component

### 4. Start the Backend

```bash
cd backend
source venv/bin/activate
python server.py
```

The server will start on `http://localhost:8000` and show:
```
âœ… Model loaded successfully (offline mode)!
Backend: MPS | Model: v1-5-pruned-emaonly-fp16
```

### 5. Start the Frontend

```bash
# In a new terminal, from project root
npm run dev
```

Open `http://localhost:5173`

The toolbar will show backend status:
- **ğŸŸ¢ MPS/CUDA/CPU** - Connected with model loaded
- **ğŸŸ¡ SIMULATION** - Backend offline, using simulation mode

## Usage

### Creating an img2img Workflow

1. **Add Input Image**: Click Assets â†’ Imported â†’ Click an image to add to canvas
2. **Add Model Node**: Click Models â†’ Click the model to add
3. **Connect Nodes**: Drag from Image output (right) â†’ Model input (left)
4. **Configure Parameters**:
   - **Positive Prompt**: What you want to see
   - **Negative Prompt**: What to avoid
   - **Steps**: Denoising steps (3-50, more = better quality)
   - **CFG Scale**: Prompt strength (1-20)
   - **Sampler**: LCM (fast), Euler, DPM++, etc.
   - **Denoise**: How much to change (0-1, higher = more change)
5. **Run**: Click **â–¶ Run** in toolbar
6. **Output**: An Output node appears automatically showing:
   - Generated image preview
   - File path in `data/output/`
   - Generation time

### Keyboard Shortcuts

| Key | Action |
|-----|--------|
| âŒ˜Z | Undo |
| â‡§âŒ˜Z | Redo |
| Delete/Backspace | Delete selected |
| Escape | Deselect all |
| Shift+Click | Multi-select |
| Space+Drag | Pan canvas |
| âŒ˜1 | Zoom to fit |
| âŒ˜+ | Zoom in |
| âŒ˜- | Zoom out |

## Node Types

| Type | Category | Description |
|------|----------|-------------|
| **Image** | Input | Source image for img2img |
| **Model** | Model | SD 1.5 img2img processor with prompts & sampler params |
| **Output** | Output | Auto-created, shows generated image and file path |

## Project Structure

```
â”œâ”€â”€ backend/              # Python inference server
â”‚   â”œâ”€â”€ server.py        # FastAPI server with Diffusers
â”‚   â”œâ”€â”€ requirements.txt # Python dependencies
â”‚   â””â”€â”€ venv/            # Python virtual environment
â”œâ”€â”€ data/                 # Local storage (gitignored contents)
â”‚   â”œâ”€â”€ input/           # Uploaded images
â”‚   â”œâ”€â”€ models/          # AI models
â”‚   â”‚   â””â”€â”€ sd-v1-5-local/  # Diffusers-format SD 1.5
â”‚   â”œâ”€â”€ output/          # Generated images (img2img_TIMESTAMP_SEED.png)
â”‚   â””â”€â”€ canvases/        # Saved workflows
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ canvas/      # WebGPU/Canvas2D rendering
â”‚   â”‚   â”œâ”€â”€ graph/       # Node graph logic & store
â”‚   â”‚   â”‚   â”œâ”€â”€ execution.ts   # Topological execution engine
â”‚   â”‚   â”‚   â”œâ”€â”€ store.svelte.ts # Yjs-backed reactive store
â”‚   â”‚   â”‚   â””â”€â”€ nodes/registry.ts # Node type definitions
â”‚   â”‚   â”œâ”€â”€ inference/   # Backend communication
â”‚   â”‚   â”‚   â””â”€â”€ manager.ts # HTTP client for img2img API
â”‚   â”‚   â”œâ”€â”€ ui/          # Svelte components
â”‚   â”‚   â””â”€â”€ workers/     # Web Workers (fallback)
â”‚   â””â”€â”€ main.ts
â””â”€â”€ index.html
```

## Backend API

### `GET /api/model/info`
Returns model status:
```json
{
  "loaded": true,
  "model_name": "v1-5-pruned-emaonly-fp16",
  "device": "mps"
}
```

### `POST /api/img2img`
Performs img2img generation. Form data:
- `image`: Input image file
- `positive_prompt`: Text prompt
- `negative_prompt`: Negative prompt
- `seed`: Random seed (int)
- `steps`: Inference steps (int)
- `cfg`: Guidance scale (float)
- `sampler_name`: Sampler type
- `scheduler`: Scheduler type
- `denoise`: Denoising strength (0-1)

Returns:
```json
{
  "image": "data:image/png;base64,...",
  "output_path": "/path/to/data/output/img2img_123456_42.png",
  "time_taken": 8.5,
  "width": 512,
  "height": 512
}
```

## Troubleshooting

### 403 WebSocket Errors in Terminal

```
WebSocket /ws?clientId=xxx" 403
connection rejected (403 Forbidden)
```

This is **harmless**. It's caused by ComfyUI (if installed) trying to connect to port 8000. Our server correctly rejects these. To stop the messages:
- Close ComfyUI app/browser tab, OR
- Run our server on a different port: `uvicorn server:app --port 8001`

### Black/Corrupt Output Images

If generated images are black, the server automatically applies a fix for MPS (Apple Silicon) by running in float32 mode. Check the server logs for:
```
Pipeline running in float32 for MPS stability
```

### Model Not Loading

1. Ensure all model files are in `data/models/sd-v1-5-local/`
2. Check file sizes match expected:
   - `unet/diffusion_pytorch_model.safetensors`: ~3.4 GB
   - `vae/diffusion_pytorch_model.safetensors`: ~335 MB
   - `text_encoder/model.safetensors`: ~492 MB
3. Check server logs for specific errors

### Simulation Mode (Yellow Indicator)

If frontend shows "SIMULATION" instead of "MPS/CUDA":
1. Ensure backend is running: `curl http://localhost:8000/api/model/info`
2. Check backend loaded model: look for "âœ… Model loaded" in server logs
3. Refresh frontend page

## Performance

| Hardware | Speed | Notes |
|----------|-------|-------|
| Apple M1/M2/M3 | ~8-12s/image | Uses MPS, float32 for stability |
| NVIDIA RTX 3080+ | ~3-5s/image | Uses CUDA, float16 |
| CPU | ~30-60s/image | Not recommended |

Tips:
- Use fewer steps (3-10 with LCM sampler)
- Lower CFG scale (2-4 with LCM)
- Use 512x512 input images

## Development

```bash
# Type checking
npm run check

# Build for production
npm run build

# Preview production build
npm run preview
```

## License

MIT
